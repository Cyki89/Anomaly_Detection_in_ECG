{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import nessesary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Dropout, TimeDistributed, CuDNNLSTM, LSTM, RepeatVector, Conv1D, MaxPool1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"./data_preprocessed/train_dataset.csv\", index_col=0)\n",
    "val_dataset = pd.read_csv(\"./data_preprocessed/val_dataset.csv\", index_col=0)\n",
    "test_dataset = pd.read_csv(\"./data_preprocessed/test_dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timiestamp_1</th>\n",
       "      <th>timiestamp_2</th>\n",
       "      <th>timiestamp_3</th>\n",
       "      <th>timiestamp_4</th>\n",
       "      <th>timiestamp_5</th>\n",
       "      <th>timiestamp_6</th>\n",
       "      <th>timiestamp_7</th>\n",
       "      <th>timiestamp_8</th>\n",
       "      <th>timiestamp_9</th>\n",
       "      <th>timiestamp_10</th>\n",
       "      <th>...</th>\n",
       "      <th>timiestamp_132</th>\n",
       "      <th>timiestamp_133</th>\n",
       "      <th>timiestamp_134</th>\n",
       "      <th>timiestamp_135</th>\n",
       "      <th>timiestamp_136</th>\n",
       "      <th>timiestamp_137</th>\n",
       "      <th>timiestamp_138</th>\n",
       "      <th>timiestamp_139</th>\n",
       "      <th>timiestamp_140</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>0.245897</td>\n",
       "      <td>-2.381731</td>\n",
       "      <td>-3.379114</td>\n",
       "      <td>-4.150560</td>\n",
       "      <td>-4.362152</td>\n",
       "      <td>-3.604735</td>\n",
       "      <td>-2.203830</td>\n",
       "      <td>-1.692911</td>\n",
       "      <td>-1.411593</td>\n",
       "      <td>-0.453160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801240</td>\n",
       "      <td>0.956382</td>\n",
       "      <td>1.052721</td>\n",
       "      <td>1.283904</td>\n",
       "      <td>1.140007</td>\n",
       "      <td>1.142146</td>\n",
       "      <td>0.833684</td>\n",
       "      <td>1.462835</td>\n",
       "      <td>1.532836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.545657</td>\n",
       "      <td>-1.014383</td>\n",
       "      <td>-2.316698</td>\n",
       "      <td>-3.634040</td>\n",
       "      <td>-4.196857</td>\n",
       "      <td>-3.758093</td>\n",
       "      <td>-3.194444</td>\n",
       "      <td>-2.221764</td>\n",
       "      <td>-1.588554</td>\n",
       "      <td>-1.202146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777530</td>\n",
       "      <td>1.119240</td>\n",
       "      <td>0.902984</td>\n",
       "      <td>0.554098</td>\n",
       "      <td>0.497053</td>\n",
       "      <td>0.418116</td>\n",
       "      <td>0.703108</td>\n",
       "      <td>1.064602</td>\n",
       "      <td>-0.044853</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>0.070699</td>\n",
       "      <td>-2.856309</td>\n",
       "      <td>-4.265050</td>\n",
       "      <td>-4.404080</td>\n",
       "      <td>-4.180707</td>\n",
       "      <td>-3.840098</td>\n",
       "      <td>-2.526704</td>\n",
       "      <td>-1.319836</td>\n",
       "      <td>-1.181694</td>\n",
       "      <td>-0.682616</td>\n",
       "      <td>...</td>\n",
       "      <td>1.168188</td>\n",
       "      <td>1.352643</td>\n",
       "      <td>1.585120</td>\n",
       "      <td>1.585385</td>\n",
       "      <td>1.309638</td>\n",
       "      <td>1.017802</td>\n",
       "      <td>0.896873</td>\n",
       "      <td>1.368133</td>\n",
       "      <td>-0.049731</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>-1.537689</td>\n",
       "      <td>-2.534511</td>\n",
       "      <td>-4.240574</td>\n",
       "      <td>-5.250626</td>\n",
       "      <td>-4.853930</td>\n",
       "      <td>-4.223230</td>\n",
       "      <td>-3.200279</td>\n",
       "      <td>-2.332330</td>\n",
       "      <td>-1.817484</td>\n",
       "      <td>-1.083945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032903</td>\n",
       "      <td>0.299982</td>\n",
       "      <td>0.707729</td>\n",
       "      <td>0.908099</td>\n",
       "      <td>1.004647</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>0.383952</td>\n",
       "      <td>0.890997</td>\n",
       "      <td>0.461981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>-0.296967</td>\n",
       "      <td>-2.149871</td>\n",
       "      <td>-3.835708</td>\n",
       "      <td>-4.670072</td>\n",
       "      <td>-4.334111</td>\n",
       "      <td>-3.239545</td>\n",
       "      <td>-2.080338</td>\n",
       "      <td>-1.665445</td>\n",
       "      <td>-1.266009</td>\n",
       "      <td>-0.374374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800871</td>\n",
       "      <td>1.086116</td>\n",
       "      <td>1.090475</td>\n",
       "      <td>0.898527</td>\n",
       "      <td>0.860276</td>\n",
       "      <td>1.536581</td>\n",
       "      <td>1.852604</td>\n",
       "      <td>0.618098</td>\n",
       "      <td>-2.105530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timiestamp_1  timiestamp_2  timiestamp_3  timiestamp_4  timiestamp_5  \\\n",
       "1603      0.245897     -2.381731     -3.379114     -4.150560     -4.362152   \n",
       "3         0.545657     -1.014383     -2.316698     -3.634040     -4.196857   \n",
       "2553      0.070699     -2.856309     -4.265050     -4.404080     -4.180707   \n",
       "269      -1.537689     -2.534511     -4.240574     -5.250626     -4.853930   \n",
       "286      -0.296967     -2.149871     -3.835708     -4.670072     -4.334111   \n",
       "\n",
       "      timiestamp_6  timiestamp_7  timiestamp_8  timiestamp_9  timiestamp_10  \\\n",
       "1603     -3.604735     -2.203830     -1.692911     -1.411593      -0.453160   \n",
       "3        -3.758093     -3.194444     -2.221764     -1.588554      -1.202146   \n",
       "2553     -3.840098     -2.526704     -1.319836     -1.181694      -0.682616   \n",
       "269      -4.223230     -3.200279     -2.332330     -1.817484      -1.083945   \n",
       "286      -3.239545     -2.080338     -1.665445     -1.266009      -0.374374   \n",
       "\n",
       "      ...  timiestamp_132  timiestamp_133  timiestamp_134  timiestamp_135  \\\n",
       "1603  ...        0.801240        0.956382        1.052721        1.283904   \n",
       "3     ...        0.777530        1.119240        0.902984        0.554098   \n",
       "2553  ...        1.168188        1.352643        1.585120        1.585385   \n",
       "269   ...       -0.032903        0.299982        0.707729        0.908099   \n",
       "286   ...        0.800871        1.086116        1.090475        0.898527   \n",
       "\n",
       "      timiestamp_136  timiestamp_137  timiestamp_138  timiestamp_139  \\\n",
       "1603        1.140007        1.142146        0.833684        1.462835   \n",
       "3           0.497053        0.418116        0.703108        1.064602   \n",
       "2553        1.309638        1.017802        0.896873        1.368133   \n",
       "269         1.004647        0.855263        0.383952        0.890997   \n",
       "286         0.860276        1.536581        1.852604        0.618098   \n",
       "\n",
       "      timiestamp_140  label  \n",
       "1603        1.532836      1  \n",
       "3          -0.044853      1  \n",
       "2553       -0.049731      1  \n",
       "269         0.461981      1  \n",
       "286        -2.105530      1  \n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Transform data for LSTM AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Drop labels from training set and convert dataframe to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2481, 140)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train_dataset.drop(columns=['label']))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Split test and validation dataset to feature and labels sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438, 140) (438,) (438, 140) (438,)\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = np.array(val_dataset.drop(columns=['label'])), np.array(val_dataset['label'])\n",
    "X_test, y_test = np.array(test_dataset.drop(columns=['label'])), np.array(test_dataset['label'])\n",
    "\n",
    "print(X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Reshape feature set to desire format for LSTM (3d array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2481, 140, 1) (438, 140, 1) (438, 140, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape to [num samples, num timesteps, num features]\n",
    "X_train = X_train.reshape(*X_train.shape,1)\n",
    "X_val = X_val.reshape(*X_val.shape,1)\n",
    "X_test = X_test.reshape(*X_test.shape,1)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Dropout, TimeDistributed, CuDNNLSTM, LSTM, RepeatVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 138, 64)           256       \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 64)                33280     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 140, 64)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 140, 64)           33280     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 140, 1)            65        \n",
      "=================================================================\n",
      "Total params: 66,881\n",
      "Trainable params: 66,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2481 samples\n",
      "Epoch 1/100\n",
      "  32/2481 [..............................] - ETA: 3:58WARNING:tensorflow:Early stopping conditioned on metric `loss` which is not available. Available metrics are: \n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential_2/conv1d_3/conv1d (defined at c:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_3463]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1f20959d3695>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# fit network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# print training history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mc:\\users\\48509\\envs\\tf\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential_2/conv1d_3/conv1d (defined at c:\\users\\48509\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_3463]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "# define early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    min_delta=0, # minimum change to \n",
    "    verbose=1,\n",
    "    patience=20,\n",
    "    mode='min',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1:]) ) ) \n",
    "model.add(CuDNNLSTM(64))\n",
    "model.add(RepeatVector(X_train.shape[1]))\n",
    "model.add(CuDNNLSTM(64, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "# fit network\n",
    "history=model.fit(X_train, X_train, epochs=100, batch_size=32, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# print training history\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on training set\n",
    "X_train_pred = model.predict(X_train)\n",
    "print(X_train_pred.shape)\n",
    "\n",
    "# plot baseline and predictions\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(X_train[i].flatten(), color='g', label='true')\n",
    "    plt.plot(X_train_pred[i].flatten(), color='r', label='predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction(reconstruction) training set\n",
    "X_train_pred = model.predict(X_train)\n",
    "\n",
    "# make prediction(reconstruction) validation set\n",
    "X_val_pred = model.predict(X_val)\n",
    "\n",
    "# make prediction(reconstruction) test set\n",
    "X_test_pred = model.predict(X_test)\n",
    "\n",
    "# calculate reconstrucion loss for each train sequence\n",
    "train_loss = [mean_absolute_error( X_train[i].flatten(), X_train_pred[i].flatten() ) for i in range(X_train.shape[0])]\n",
    "\n",
    "# calculate reconstrucion loss for each val sequence\n",
    "val_loss = [mean_absolute_error( X_val[i].flatten(), X_val_pred[i].flatten() ) for i in range(X_val.shape[0])]\n",
    "\n",
    "# calculate reconstrucion loss for each test sequence\n",
    "test_loss = [mean_absolute_error( X_test[i].flatten(), X_test_pred[i].flatten() ) for i in range(X_test.shape[0])]\n",
    "\n",
    "# show train reconstrucion losses distribution\n",
    "sns.distplot(train_loss, bins=50, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define percentile range for tesing threshold\n",
    "percentiles = range(90,100)\n",
    "\n",
    "# define threshold variable\n",
    "THRESHOLD = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for percentile in percentiles:\n",
    "    # set theshold based on percentile on train reconctrucion losses\n",
    "    testing_threshold = np.percentile(train_loss, percentile)\n",
    "    \n",
    "    # predict validation set classes based on validation reconctrucion losses and testing threshold\n",
    "    y_val_pred= [int(x < testing_threshold) for x in val_loss]\n",
    "    y_test_pred = [int(x < testing_threshold) for x in test_loss]\n",
    "    \n",
    "    # calculate validaion accuracy\n",
    "    val_accuracy = accuracy_score(y_val_pred, y_val)\n",
    "    test_accuracy = accuracy_score(y_test_pred, y_test)\n",
    "    \n",
    "    # compare current validation accuracy with best accuracy\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        THRESHOLD = testing_threshold\n",
    "    \n",
    "    print((f'Perentile:{percentile} | Threshold: {round(testing_threshold,4)} ' \n",
    "           f'| Val Accuracy: {round(val_accuracy, 4)} | '\n",
    "           f'Test Accuracy: {round(test_accuracy ,4)} ') )\n",
    "\n",
    "print('-'*50)\n",
    "print(f'Best validation accuracy: {round(best_accuracy,4)} for Threshold {round(THRESHOLD,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test set classes based on test reconctrucion losses and selected threshold\n",
    "y_test_pred = [int(x < THRESHOLD) for x in test_loss]\n",
    "\n",
    "# calculate test set accuracy prediction\n",
    "test_accuracy = accuracy_score(y_test_pred, y_test)\n",
    "\n",
    "print(round(test_accuracy,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(data, model, title, ax):\n",
    "    predictions = model.predict(data)\n",
    "    pred_losses = mean_absolute_error(predictions.flatten(), data.flatten())\n",
    "    ax.plot(data.flatten(), label='true')\n",
    "    ax.plot(predictions.flatten(), label='reconstructed')\n",
    "    ax.set_title(f'{title} (loss: {np.around(pred_losses, 2)})')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot some  normal and anomaly sample prediction from test set\n",
    "X_test_normal = X_test[y_test==1]\n",
    "X_test_anomaly = X_test[y_test==0]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "  nrows=2,\n",
    "  ncols=6,\n",
    "  sharey=True,\n",
    "  sharex=True,\n",
    "  figsize=(22, 8)\n",
    ")\n",
    "\n",
    "for i in range(6):\n",
    "    plot_prediction(X_test_normal[i:i+1], model, title='Normal', ax=axs[0, i])\n",
    "\n",
    "for i in range(6):\n",
    "    plot_prediction(X_test_anomaly[i:i+1], model, title='Anomaly', ax=axs[1, i])\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Scaling data with MinMaxScaler(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "class MultipleColumnScaler(BaseEstimator, TransformerMixin):\n",
    "    ''' take multiple columns and scaling it's keeping original ratio between them '''\n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        columns_merged = X[:,0]\n",
    "        for i in range(1, X.shape[1]):\n",
    "            columns_merged = np.concatenate((columns_merged, X[:,i]), axis=0)\n",
    "        self.scaler.fit(columns_merged.reshape(-1,1))\n",
    "        return self.scaler\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_new = self.scaler.transform(X[:, 0].reshape(-1,1))\n",
    "        for i in range(1, X.shape[1]):\n",
    "            X_curr = self.scaler.transform(X[:, i].reshape(-1,1))\n",
    "            X_new = np.concatenate((X_new, X_curr), axis=1)\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_dataset.drop(columns=['label']))\n",
    "print(X_train.shape)\n",
    "\n",
    "X_val, y_val = np.array(val_dataset.drop(columns=['label'])), np.array(val_dataset['label'])\n",
    "X_test, y_test = np.array(test_dataset.drop(columns=['label'])), np.array(test_dataset['label'])\n",
    "\n",
    "print(X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "X_scaler = MultipleColumnScaler(MinMaxScaler(feature_range=(-1,1) ) )\n",
    "X_scaler = X_scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_val_scaled = X_scaler.transform(X_val)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = X_train_scaled.reshape(*X_train_scaled.shape,1)\n",
    "X_val_scaled = X_val_scaled.reshape(*X_val_scaled.shape,1)\n",
    "X_test_scaled = X_test_scaled.reshape(*X_test_scaled.shape,1)\n",
    "\n",
    "# reshape to [num samples, num timesteps, num features]\n",
    "X_train = X_train.reshape(*X_train.shape,1)\n",
    "X_val = X_val.reshape(*X_val.shape,1)\n",
    "X_test = X_test.reshape(*X_test.shape,1)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(128, input_shape=(X_train_scaled.shape[1:]), return_sequences=True))\n",
    "model.add(RepeatVector(X_train.shape[1]))\n",
    "model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "# fit network\n",
    "history=model.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# print training history\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on training set\n",
    "X_train_scaled_pred = model.predict(X_train_scaled)\n",
    "\n",
    "# inverse scaling\n",
    "X_train_inv_pred = X_scaler.inverse_transform(X_train_scaled_pred.reshape(X_train_scaled_pred.shape[:2]))\n",
    "\n",
    "# again reshape to 3d array \n",
    "X_train_inv_pred = X_train_inv_pred.reshape(*X_train_inv_pred.shape,1)\n",
    "\n",
    "# plot baseline and predictions\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(X_train[i].flatten(), color='g', label='true')\n",
    "    plt.plot(X_train_inv_pred[i].flatten(), color='r', label='predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction(reconstruction) training set\n",
    "X_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "# make prediction(reconstruction) validation set\n",
    "X_val_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# make prediction(reconstruction) test set\n",
    "X_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# inverse scaling \n",
    "X_train_pred = X_scaler.inverse_transform(X_train_pred.reshape(X_train_pred.shape[:2]))\n",
    "X_val_pred = X_scaler.inverse_transform(X_val_pred.reshape(X_val_pred.shape[:2]))\n",
    "X_test_pred = X_scaler.inverse_transform(X_test_pred.reshape(X_test_pred.shape[:2]))\n",
    "\n",
    "# again reshape to 3d array \n",
    "X_train_pred = X_train_pred.reshape(*X_train_pred.shape,1)\n",
    "X_val_pred = X_val_pred.reshape(*X_val_pred.shape,1)\n",
    "X_test_pred = X_test_pred.reshape(*X_test_pred.shape,1)\n",
    "\n",
    "# calculate reconstrucion loss for each train sequence\n",
    "train_loss = [mean_absolute_error( X_train[i].flatten(), X_train_pred[i].flatten() ) for i in range(X_train.shape[0])]\n",
    "\n",
    "# calculate reconstrucion loss for each val sequence\n",
    "val_loss = [mean_absolute_error( X_val[i].flatten(), X_val_pred[i].flatten() ) for i in range(X_val.shape[0])]\n",
    "\n",
    "# calculate reconstrucion loss for each test sequence\n",
    "test_loss = [mean_absolute_error( X_test[i].flatten(), X_test_pred[i].flatten() ) for i in range(X_test.shape[0])]\n",
    "\n",
    "# show train reconstrucion losses distribution\n",
    "sns.distplot(train_loss, bins=50, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define percentile range for tesing threshold\n",
    "percentiles = range(90,100)\n",
    "\n",
    "# define threshold variable\n",
    "THRESHOLD = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for percentile in percentiles:\n",
    "    # set theshold based on percentile on train reconctrucion losses\n",
    "    testing_threshold = np.percentile(train_loss, percentile)\n",
    "    \n",
    "    # predict validation set classes based on validation reconctrucion losses and testing threshold\n",
    "    y_val_pred= [int(x < testing_threshold) for x in val_loss]\n",
    "    y_test_pred = [int(x < testing_threshold) for x in test_loss]\n",
    "    \n",
    "    # calculate validaion accuracy\n",
    "    val_accuracy = accuracy_score(y_val_pred, y_val)\n",
    "    test_accuracy = accuracy_score(y_test_pred, y_test)\n",
    "    \n",
    "    # compare current validation accuracy with best accuracy\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        THRESHOLD = testing_threshold\n",
    "    \n",
    "    print((f'Perentile:{percentile} | Threshold: {round(testing_threshold,4)} ' \n",
    "           f'| Val Accuracy: {round(val_accuracy, 4)} | '\n",
    "           f'Test Accuracy: {round(test_accuracy ,4)} ') )\n",
    "\n",
    "print('-'*50)\n",
    "print(f'Best validation accuracy: {round(best_accuracy,4)} for Threshold {round(THRESHOLD,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test set classes based on test reconctrucion losses and selected threshold\n",
    "y_test_pred = [int(x < THRESHOLD) for x in test_loss]\n",
    "\n",
    "# calculate test set accuracy prediction\n",
    "test_accuracy = accuracy_score(y_test_pred, y_test)\n",
    "\n",
    "print(round(test_accuracy,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(data, data_scaled, X_scaler, model, title, ax):\n",
    "    predictions = model.predict(data_scaled)\n",
    "    predictions = X_scaler.inverse_transform(predictions.reshape(predictions.shape[:2]))\n",
    "    pred_losses = mean_absolute_error(predictions.flatten(), data.flatten())\n",
    "    ax.plot(data.flatten(), label='true')\n",
    "    ax.plot(predictions.flatten(), label='reconstructed')\n",
    "    ax.set_title(f'{title} (loss: {np.around(pred_losses, 2)})')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot some  normal and anomaly sample prediction from test set\n",
    "X_test_normal_scaled = X_test_scaled[y_test==1]\n",
    "X_test_anomaly_scaled = X_test_scaled[y_test==0]\n",
    "X_test_normal = X_test[y_test==1]\n",
    "X_test_anomaly = X_test[y_test==0]\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "  nrows=2,\n",
    "  ncols=6,\n",
    "  sharey=True,\n",
    "  sharex=True,\n",
    "  figsize=(22, 8)\n",
    ")\n",
    "\n",
    "for i in range(6):\n",
    "    plot_prediction(X_test_normal[i:i+1], X_test_normal_scaled[i:i+1], X_scaler, model, title='Normal', ax=axs[0, i])\n",
    "\n",
    "for i in range(6):\n",
    "    plot_prediction(X_test_anomaly[i:i+1], X_test_anomaly_scaled[i:i+1], X_scaler, model, title='Anomaly', ax=axs[1, i])\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
