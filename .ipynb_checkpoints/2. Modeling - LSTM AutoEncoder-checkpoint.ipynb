{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import nessesary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Dropout, TimeDistributed, CuDNNLSTM, LSTM, RepeatVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"./data_preprocessed/train_dataset.csv\", index_col=0)\n",
    "val_dataset = pd.read_csv(\"./data_preprocessed/val_dataset.csv\", index_col=0)\n",
    "test_dataset = pd.read_csv(\"./data_preprocessed/test_dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timiestamp_1</th>\n",
       "      <th>timiestamp_2</th>\n",
       "      <th>timiestamp_3</th>\n",
       "      <th>timiestamp_4</th>\n",
       "      <th>timiestamp_5</th>\n",
       "      <th>timiestamp_6</th>\n",
       "      <th>timiestamp_7</th>\n",
       "      <th>timiestamp_8</th>\n",
       "      <th>timiestamp_9</th>\n",
       "      <th>timiestamp_10</th>\n",
       "      <th>...</th>\n",
       "      <th>timiestamp_132</th>\n",
       "      <th>timiestamp_133</th>\n",
       "      <th>timiestamp_134</th>\n",
       "      <th>timiestamp_135</th>\n",
       "      <th>timiestamp_136</th>\n",
       "      <th>timiestamp_137</th>\n",
       "      <th>timiestamp_138</th>\n",
       "      <th>timiestamp_139</th>\n",
       "      <th>timiestamp_140</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>0.245897</td>\n",
       "      <td>-2.381731</td>\n",
       "      <td>-3.379114</td>\n",
       "      <td>-4.150560</td>\n",
       "      <td>-4.362152</td>\n",
       "      <td>-3.604735</td>\n",
       "      <td>-2.203830</td>\n",
       "      <td>-1.692911</td>\n",
       "      <td>-1.411593</td>\n",
       "      <td>-0.453160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801240</td>\n",
       "      <td>0.956382</td>\n",
       "      <td>1.052721</td>\n",
       "      <td>1.283904</td>\n",
       "      <td>1.140007</td>\n",
       "      <td>1.142146</td>\n",
       "      <td>0.833684</td>\n",
       "      <td>1.462835</td>\n",
       "      <td>1.532836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.545657</td>\n",
       "      <td>-1.014383</td>\n",
       "      <td>-2.316698</td>\n",
       "      <td>-3.634040</td>\n",
       "      <td>-4.196857</td>\n",
       "      <td>-3.758093</td>\n",
       "      <td>-3.194444</td>\n",
       "      <td>-2.221764</td>\n",
       "      <td>-1.588554</td>\n",
       "      <td>-1.202146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777530</td>\n",
       "      <td>1.119240</td>\n",
       "      <td>0.902984</td>\n",
       "      <td>0.554098</td>\n",
       "      <td>0.497053</td>\n",
       "      <td>0.418116</td>\n",
       "      <td>0.703108</td>\n",
       "      <td>1.064602</td>\n",
       "      <td>-0.044853</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>0.070699</td>\n",
       "      <td>-2.856309</td>\n",
       "      <td>-4.265050</td>\n",
       "      <td>-4.404080</td>\n",
       "      <td>-4.180707</td>\n",
       "      <td>-3.840098</td>\n",
       "      <td>-2.526704</td>\n",
       "      <td>-1.319836</td>\n",
       "      <td>-1.181694</td>\n",
       "      <td>-0.682616</td>\n",
       "      <td>...</td>\n",
       "      <td>1.168188</td>\n",
       "      <td>1.352643</td>\n",
       "      <td>1.585120</td>\n",
       "      <td>1.585385</td>\n",
       "      <td>1.309638</td>\n",
       "      <td>1.017802</td>\n",
       "      <td>0.896873</td>\n",
       "      <td>1.368133</td>\n",
       "      <td>-0.049731</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>-1.537689</td>\n",
       "      <td>-2.534511</td>\n",
       "      <td>-4.240574</td>\n",
       "      <td>-5.250626</td>\n",
       "      <td>-4.853930</td>\n",
       "      <td>-4.223230</td>\n",
       "      <td>-3.200279</td>\n",
       "      <td>-2.332330</td>\n",
       "      <td>-1.817484</td>\n",
       "      <td>-1.083945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032903</td>\n",
       "      <td>0.299982</td>\n",
       "      <td>0.707729</td>\n",
       "      <td>0.908099</td>\n",
       "      <td>1.004647</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>0.383952</td>\n",
       "      <td>0.890997</td>\n",
       "      <td>0.461981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>-0.296967</td>\n",
       "      <td>-2.149871</td>\n",
       "      <td>-3.835708</td>\n",
       "      <td>-4.670072</td>\n",
       "      <td>-4.334111</td>\n",
       "      <td>-3.239545</td>\n",
       "      <td>-2.080338</td>\n",
       "      <td>-1.665445</td>\n",
       "      <td>-1.266009</td>\n",
       "      <td>-0.374374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800871</td>\n",
       "      <td>1.086116</td>\n",
       "      <td>1.090475</td>\n",
       "      <td>0.898527</td>\n",
       "      <td>0.860276</td>\n",
       "      <td>1.536581</td>\n",
       "      <td>1.852604</td>\n",
       "      <td>0.618098</td>\n",
       "      <td>-2.105530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timiestamp_1  timiestamp_2  timiestamp_3  timiestamp_4  timiestamp_5  \\\n",
       "1603      0.245897     -2.381731     -3.379114     -4.150560     -4.362152   \n",
       "3         0.545657     -1.014383     -2.316698     -3.634040     -4.196857   \n",
       "2553      0.070699     -2.856309     -4.265050     -4.404080     -4.180707   \n",
       "269      -1.537689     -2.534511     -4.240574     -5.250626     -4.853930   \n",
       "286      -0.296967     -2.149871     -3.835708     -4.670072     -4.334111   \n",
       "\n",
       "      timiestamp_6  timiestamp_7  timiestamp_8  timiestamp_9  timiestamp_10  \\\n",
       "1603     -3.604735     -2.203830     -1.692911     -1.411593      -0.453160   \n",
       "3        -3.758093     -3.194444     -2.221764     -1.588554      -1.202146   \n",
       "2553     -3.840098     -2.526704     -1.319836     -1.181694      -0.682616   \n",
       "269      -4.223230     -3.200279     -2.332330     -1.817484      -1.083945   \n",
       "286      -3.239545     -2.080338     -1.665445     -1.266009      -0.374374   \n",
       "\n",
       "      ...  timiestamp_132  timiestamp_133  timiestamp_134  timiestamp_135  \\\n",
       "1603  ...        0.801240        0.956382        1.052721        1.283904   \n",
       "3     ...        0.777530        1.119240        0.902984        0.554098   \n",
       "2553  ...        1.168188        1.352643        1.585120        1.585385   \n",
       "269   ...       -0.032903        0.299982        0.707729        0.908099   \n",
       "286   ...        0.800871        1.086116        1.090475        0.898527   \n",
       "\n",
       "      timiestamp_136  timiestamp_137  timiestamp_138  timiestamp_139  \\\n",
       "1603        1.140007        1.142146        0.833684        1.462835   \n",
       "3           0.497053        0.418116        0.703108        1.064602   \n",
       "2553        1.309638        1.017802        0.896873        1.368133   \n",
       "269         1.004647        0.855263        0.383952        0.890997   \n",
       "286         0.860276        1.536581        1.852604        0.618098   \n",
       "\n",
       "      timiestamp_140  label  \n",
       "1603        1.532836      1  \n",
       "3          -0.044853      1  \n",
       "2553       -0.049731      1  \n",
       "269         0.461981      1  \n",
       "286        -2.105530      1  \n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Transform data for LSTM AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Drop labels from training set and convert dataframe to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2481, 140)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train_dataset.drop(columns=['label']))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Split test and validation dataset to feature and labels sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438, 140) (438,) (438, 140) (438,)\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = np.array(val_dataset.drop(columns=['label'])), np.array(val_dataset['label'])\n",
    "X_test, y_test = np.array(test_dataset.drop(columns=['label'])), np.array(test_dataset['label'])\n",
    "\n",
    "print(X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Reshape feature set to desire format for LSTM (3d array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2481, 140, 1) (438, 140, 1) (438, 140, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape to [num samples, num timesteps, num features]\n",
    "X_train = X_train.reshape(*X_train.shape,1)\n",
    "X_val = X_val.reshape(*X_val.shape,1)\n",
    "X_test = X_test.reshape(*X_test.shape,1)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Dropout, TimeDistributed, CuDNNLSTM, LSTM, RepeatVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm (CuDNNLSTM)       (None, 140, 128)          67072     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 64)                49664     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 140, 64)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 140, 64)           33280     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 140, 128)          99328     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 140, 1)            129       \n",
      "=================================================================\n",
      "Total params: 249,473\n",
      "Trainable params: 249,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    min_delta=0, # minimum change to \n",
    "    verbose=1,\n",
    "    patience=20,\n",
    "    mode='min',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
    "model.add(RepeatVector(X_train.shape[1]))\n",
    "model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "# fit network\n",
    "history=model.fit(X_train, X_train, epochs=100, batch_size=32, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "# print training history\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on training set\n",
    "X_train_pred = model.predict(X_train)\n",
    "print(X_train_pred.shape)\n",
    "\n",
    "# plot baseline and predictions\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(X_train[i].flatten(), color='g', label='true')\n",
    "    plt.plot(X_train_pred[i].flatten(), color='r', label='predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction(reconstruction) training set\n",
    "X_train_pred = model.predict(X_train)\n",
    "\n",
    "# make prediction(reconstruction) validation set\n",
    "X_val_pred = model.predict(X_val)\n",
    "\n",
    "# make prediction(reconstruction) test set\n",
    "X_test_pred = model.predict(X_test)\n",
    "\n",
    "# calculate reconstrucion loss for each train sequence\n",
    "train_loss = [mean_absolute_error( X_train[i].flatten(), X_train_pred[i].flatten() ) for i in range(X_train.shape[0])]\n",
    "\n",
    "# calculate reconstrucion loss for each val sequence\n",
    "val_loss = [mean_absolute_error( X_val[i].flatten(), X_val_pred[i].flatten() ) for i in range(X_val.shape[0])]\n",
    "\n",
    "# calculate reconstrucion loss for each test sequence\n",
    "test_loss = [mean_absolute_error( X_test[i].flatten(), X_test_pred[i].flatten() ) for i in range(X_test.shape[0])]\n",
    "\n",
    "# show train reconstrucion losses distribution\n",
    "sns.distplot(train_loss, bins=50, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define percentile range for tesing threshold\n",
    "percentiles = range(90,100)\n",
    "\n",
    "# define threshold variable\n",
    "THRESHOLD = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for percentile in percentiles:\n",
    "    # set theshold based on percentile on train reconctrucion losses\n",
    "    testing_threshold = np.percentile(train_loss, percentile)\n",
    "    \n",
    "    # predict validation set classes based on validation reconctrucion losses and testing threshold\n",
    "    y_val_pred= [int(x < testing_threshold) for x in val_loss]\n",
    "    y_test_pred = [int(x < testing_threshold) for x in test_loss]\n",
    "    \n",
    "    # calculate validaion accuracy\n",
    "    val_accuracy = accuracy_score(y_val_pred, y_val)\n",
    "    test_accuracy = accuracy_score(y_test_pred, y_test)\n",
    "    \n",
    "    # compare current validation accuracy with best accuracy\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        THRESHOLD = testing_threshold\n",
    "    \n",
    "    print((f'Perentile:{percentile} | Threshold: {round(testing_threshold,4)} ' \n",
    "           f'| Val Accuracy: {round(val_accuracy, 4)} | '\n",
    "           f'Test Accuracy: {round(test_accuracy ,4)} ') )\n",
    "\n",
    "print('-'*50)\n",
    "print(f'Best validation accuracy: {round(best_accuracy,4)} for Threshold {round(THRESHOLD,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test set classes based on test reconctrucion losses and selected threshold\n",
    "y_test_pred = [int(x < THRESHOLD) for x in test_loss]\n",
    "\n",
    "# calculate test set accuracy prediction\n",
    "test_accuracy = accuracy_score(y_test_pred, y_test)\n",
    "\n",
    "print(round(test_accuracy,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(data, model, title, ax):\n",
    "    predictions = model.predict(data)\n",
    "    pred_losses = mean_absolute_error(predictions.flatten(), data.flatten())\n",
    "    ax.plot(data.flatten(), label='true')\n",
    "    ax.plot(predictions.flatten(), label='reconstructed')\n",
    "    ax.set_title(f'{title} (loss: {np.around(pred_losses, 2)})')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot some  normal and anomaly sample prediction from test set\n",
    "X_test_normal = X_test[y_test==1]\n",
    "X_test_anomaly = X_test[y_test==0]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "  nrows=2,\n",
    "  ncols=6,\n",
    "  sharey=True,\n",
    "  sharex=True,\n",
    "  figsize=(22, 8)\n",
    ")\n",
    "\n",
    "for i in range(6):\n",
    "    plot_prediction(X_test_normal[i:i+1], model, title='Normal', ax=axs[0, i])\n",
    "\n",
    "for i in range(6):\n",
    "    plot_prediction(X_test_anomaly[i:i+1], model, title='Anomaly', ax=axs[1, i])\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Scaling data with MinMaxScaler(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "class MultipleColumnScaler(BaseEstimator, TransformerMixin):\n",
    "    ''' take multiple columns and scaling it's keeping original ratio between them '''\n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        columns_merged = X[:,0]\n",
    "        for i in range(1, X.shape[1]):\n",
    "            columns_merged = np.concatenate((columns_merged, X[:,i]), axis=0)\n",
    "        self.scaler.fit(columns_merged.reshape(-1,1))\n",
    "        return self.scaler\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_new = self.scaler.transform(X[:, 0].reshape(-1,1))\n",
    "        for i in range(1, X.shape[1]):\n",
    "            X_curr = self.scaler.transform(X[:, i].reshape(-1,1))\n",
    "            X_new = np.concatenate((X_new, X_curr), axis=1)\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_dataset.drop(columns=['label']))\n",
    "print(X_train.shape)\n",
    "\n",
    "X_val, y_val = np.array(val_dataset.drop(columns=['label'])), np.array(val_dataset['label'])\n",
    "X_test, y_test = np.array(test_dataset.drop(columns=['label'])), np.array(test_dataset['label'])\n",
    "\n",
    "print(X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "X_scaler = MultipleColumnScaler(MinMaxScaler(feature_range=(-1,1) ) )\n",
    "X_scaler = X_scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_val_scaled = X_scaler.transform(X_val)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = X_train_scaled.reshape(*X_train_scaled.shape,1)\n",
    "X_val_scaled = X_val_scaled.reshape(*X_val_scaled.shape,1)\n",
    "X_test_scaled = X_test_scaled.reshape(*X_test_scaled.shape,1)\n",
    "\n",
    "# reshape to [num samples, num timesteps, num features]\n",
    "X_train = X_train.reshape(*X_train.shape,1)\n",
    "X_val = X_val.reshape(*X_val.shape,1)\n",
    "X_test = X_test.reshape(*X_test.shape,1)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(128, input_shape=(X_train_scaled.shape[1:]), return_sequences=True))\n",
    "model.add(RepeatVector(X_train.shape[1]))\n",
    "model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "# fit network\n",
    "history=model.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# print training history\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on training set\n",
    "X_train_scaled_pred = model.predict(X_train_scaled)\n",
    "\n",
    "# inverse scaling\n",
    "X_train_inv_pred = X_scaler.inverse_transform(X_train_scaled_pred.reshape(X_train_scaled_pred.shape[:2]))\n",
    "\n",
    "# again reshape to 3d array \n",
    "X_train_inv_pred = X_train_inv_pred.reshape(*X_train_inv_pred.shape,1)\n",
    "\n",
    "# plot baseline and predictions\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(X_train[i].flatten(), color='g', label='true')\n",
    "    plt.plot(X_train_inv_pred[i].flatten(), color='r', label='predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction(reconstruction) training set\n",
    "X_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "# make prediction(reconstruction) validation set\n",
    "X_val_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# make prediction(reconstruction) test set\n",
    "X_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# inverse scaling \n",
    "X_train_pred = X_scaler.inverse_transform(X_train_pred.reshape(X_train_pred.shape[:2]))\n",
    "X_val_pred = X_scaler.inverse_transform(X_val_pred.reshape(X_val_pred.shape[:2]))\n",
    "X_test_pred = X_scaler.inverse_transform(X_test_pred.reshape(X_test_pred.shape[:2]))\n",
    "\n",
    "# again reshape to 3d array \n",
    "X_train_pred = X_train_pred.reshape(*X_train_pred.shape,1)\n",
    "X_val_pred = X_val_pred.reshape(*X_val_pred.shape,1)\n",
    "X_test_pred = X_test_pred.reshape(*X_test_pred.shape,1)\n",
    "\n",
    "# calculate reconstrucion loss for each train sequence\n",
    "train_loss = [mean_absolute_error( X_train[i].flatten(), X_train_pred[i].flatten() ) for i in range(X_train.shape[0])]\n",
    "\n",
    "# calculate reconstrucion loss for each val sequence\n",
    "val_loss = [mean_absolute_error( X_val[i].flatten(), X_val_pred[i].flatten() ) for i in range(X_val.shape[0])]\n",
    "\n",
    "# calculate reconstrucion loss for each test sequence\n",
    "test_loss = [mean_absolute_error( X_test[i].flatten(), X_test_pred[i].flatten() ) for i in range(X_test.shape[0])]\n",
    "\n",
    "# show train reconstrucion losses distribution\n",
    "sns.distplot(train_loss, bins=50, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define percentile range for tesing threshold\n",
    "percentiles = range(90,100)\n",
    "\n",
    "# define threshold variable\n",
    "THRESHOLD = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for percentile in percentiles:\n",
    "    # set theshold based on percentile on train reconctrucion losses\n",
    "    testing_threshold = np.percentile(train_loss, percentile)\n",
    "    \n",
    "    # predict validation set classes based on validation reconctrucion losses and testing threshold\n",
    "    y_val_pred= [int(x < testing_threshold) for x in val_loss]\n",
    "    y_test_pred = [int(x < testing_threshold) for x in test_loss]\n",
    "    \n",
    "    # calculate validaion accuracy\n",
    "    val_accuracy = accuracy_score(y_val_pred, y_val)\n",
    "    test_accuracy = accuracy_score(y_test_pred, y_test)\n",
    "    \n",
    "    # compare current validation accuracy with best accuracy\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        THRESHOLD = testing_threshold\n",
    "    \n",
    "    print((f'Perentile:{percentile} | Threshold: {round(testing_threshold,4)} ' \n",
    "           f'| Val Accuracy: {round(val_accuracy, 4)} | '\n",
    "           f'Test Accuracy: {round(test_accuracy ,4)} ') )\n",
    "\n",
    "print('-'*50)\n",
    "print(f'Best validation accuracy: {round(best_accuracy,4)} for Threshold {round(THRESHOLD,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test set classes based on test reconctrucion losses and selected threshold\n",
    "y_test_pred = [int(x < THRESHOLD) for x in test_loss]\n",
    "\n",
    "# calculate test set accuracy prediction\n",
    "test_accuracy = accuracy_score(y_test_pred, y_test)\n",
    "\n",
    "print(round(test_accuracy,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(data, data_scaled, X_scaler, model, title, ax):\n",
    "    predictions = model.predict(data_scaled)\n",
    "    predictions = X_scaler.inverse_transform(predictions.reshape(predictions.shape[:2]))\n",
    "    pred_losses = mean_absolute_error(predictions.flatten(), data.flatten())\n",
    "    ax.plot(data.flatten(), label='true')\n",
    "    ax.plot(predictions.flatten(), label='reconstructed')\n",
    "    ax.set_title(f'{title} (loss: {np.around(pred_losses, 2)})')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot some  normal and anomaly sample prediction from test set\n",
    "X_test_normal_scaled = X_test_scaled[y_test==1]\n",
    "X_test_anomaly_scaled = X_test_scaled[y_test==0]\n",
    "X_test_normal = X_test[y_test==1]\n",
    "X_test_anomaly = X_test[y_test==0]\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "  nrows=2,\n",
    "  ncols=6,\n",
    "  sharey=True,\n",
    "  sharex=True,\n",
    "  figsize=(22, 8)\n",
    ")\n",
    "\n",
    "for i in range(6):\n",
    "    plot_prediction(X_test_normal[i:i+1], X_test_normal_scaled[i:i+1], X_scaler, model, title='Normal', ax=axs[0, i])\n",
    "\n",
    "for i in range(6):\n",
    "    plot_prediction(X_test_anomaly[i:i+1], X_test_anomaly_scaled[i:i+1], X_scaler, model, title='Anomaly', ax=axs[1, i])\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
